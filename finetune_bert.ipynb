{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e0670ad",
   "metadata": {},
   "source": [
    "# Google Colab-compatible script to preprocess comment data and fine-tune BERT MLM.\n",
    "\n",
    "### Summary:\n",
    "- Mounts Google Drive\n",
    "- Cleans and filters a directory of JSON files containing comment threads\n",
    "- Splits into train/test\n",
    "- Saves test data\n",
    "- Fine-tunes a BERT masked language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c2e941",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf416f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM, AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4cedfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive to access datasets and save outputs\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf8c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global vars\n",
    "sentencelist = []  # Holds cleaned data\n",
    "count = 0\n",
    "TEST_SAMPLE_SIZE = 500  # Number of test samples\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def clean_data(text):\n",
    "    \"\"\"\n",
    "    Cleans input text:\n",
    "    - Removes unwanted patterns\n",
    "    - Filters long/token-heavy/short sentences\n",
    "    - Adds punctuation where needed\n",
    "    \"\"\"\n",
    "    sentence_splitter = re.compile(r'[.!?] |\\n')\n",
    "    sentences = re.split(sentence_splitter, text)\n",
    "    cleaned_text = \"\"\n",
    "    pattern1 = re.compile(r\"[<>{}]|[0-9][0-9]:|[0-9]+x[0-9]+|::|:[0-9]|@@\")\n",
    "    pattern2 = re.compile(r\"^(.*/)([^/]*) |==|--|@[A-Za-z0-9]+\")\n",
    "    for sentence in sentences:\n",
    "        if not re.findall(pattern1, sentence):\n",
    "            sentence = re.sub(pattern2, '', sentence)\n",
    "            tokens = tokenizer.tokenize(sentence)\n",
    "            if 8 < len(sentence) <= 512 and len(tokens) <= 512:\n",
    "                cleaned_text += sentence if sentence.endswith('.') else sentence + \". \"\n",
    "    return cleaned_text\n",
    "\n",
    "def summarize_ticket(json_file):\n",
    "    \"\"\"\n",
    "    Processes a single JSON file:\n",
    "    - Extracts first comment\n",
    "    - Cleans the comment\n",
    "    - Appends to training data list if valid\n",
    "    \"\"\"\n",
    "    global count, sentencelist\n",
    "    data = json.load(json_file)\n",
    "    url_pattern = re.compile(r\"https?://[^\\s]+\")\n",
    "    comment = data['comments'][0]['raw_text']\n",
    "    comment = re.sub(url_pattern, \"\", comment)\n",
    "    cleaned = clean_data(comment)\n",
    "    count += 1\n",
    "    if cleaned:\n",
    "        sentencelist.append(cleaned)\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main runner:\n",
    "    - Loads JSONs\n",
    "    - Saves test data\n",
    "    - Fine-tunes BERT\n",
    "    \"\"\"\n",
    "    # Path config for Colab\n",
    "    base_dir = \"/content/drive/MyDrive/bert_project\"\n",
    "    json_dir = os.path.join(base_dir, \"comments\")\n",
    "    test_dir = os.path.join(base_dir, \"test_json\")\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    # Preprocess all files\n",
    "    for file in os.listdir(json_dir):\n",
    "        with open(os.path.join(json_dir, file), encoding=\"utf-8\") as json_file:\n",
    "            summarize_ticket(json_file)\n",
    "    print(\"Total files processed:\", count)\n",
    "\n",
    "    # Train-test split\n",
    "    train_data, test_data = sentencelist[:-TEST_SAMPLE_SIZE], sentencelist[-TEST_SAMPLE_SIZE:]\n",
    "\n",
    "    # Save test samples\n",
    "    for i, item in enumerate(test_data):\n",
    "        with open(os.path.join(test_dir, f\"test_sample_{i}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\"text\": item}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Load model\n",
    "    model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "    model.train()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Training setup\n",
    "    dataloader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5, correct_bias=False)\n",
    "    max_len = 82\n",
    "    max_norm = 1.0\n",
    "    epochs = 10\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            encoded = tokenizer.batch_encode_plus(\n",
    "                batch,\n",
    "                add_special_tokens=False,\n",
    "                max_length=max_len,\n",
    "                pad_to_max_length=True,\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt')\n",
    "            input_ids = encoded['input_ids'].to(device)\n",
    "            loss = model(input_ids, labels=input_ids)\n",
    "            loss[0].backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        print(f\"Epoch {epoch + 1} completed with loss {loss[0].item():.4f}\")\n",
    "\n",
    "    # Save trained model to Drive\n",
    "    model.save_pretrained(os.path.join(base_dir, \"Fine_Tuned_BertForMaskedLM\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
